{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4562567f",
   "metadata": {},
   "source": [
    "## Demonstration of Bivariate Linear Regression\n",
    "\n",
    "This jupyter notebook is part of a [collection of notebooks](../index.ipynb) on various topics of Digital Signal Processing. Please direct questions and suggestions to [Sascha.Spors@uni-rostock.de](mailto:Sascha.Spors@uni-rostock.de)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c0b26",
   "metadata": {},
   "source": [
    "In order to demonstrate multivariate linear regression, in this notebook the bivariate case is considered for the ease of illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5d20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65974c",
   "metadata": {},
   "source": [
    "The [Jupyter interactive widgets framework](https://github.com/matplotlib/ipympl) `ipympl` is used for interactive visualization of the two-dimensional dataset. This may require the installation of the `ipympl` package using e.g. conda or pip. Remove the following [magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) if the package is not installed or if you don't want to use interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33dd3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbdb51",
   "metadata": {},
   "source": [
    "### Generate Dataset\n",
    "\n",
    "In the following, a synthetic dataset with $N$ examples is generated by implementing a simple two-dimensional linear relationship and additive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0160b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200  # total number of examples\n",
    "alpha = 1.2  # true intercept\n",
    "theta = [0.1, 0.25]  # true slopes\n",
    "\n",
    "X = np.random.uniform(low=-5, high=10, size=(N,2))\n",
    "Y = alpha + np.dot(X, theta) + .5 * np.random.normal(size=(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5b0fd",
   "metadata": {},
   "source": [
    "The data points are plotted in order to investigate the structure of the dataset. The linear relationship becomes clearly visible when inspecting the plot from different viewpoints. The viewpoint can be changed by moving the pointer in the plot with pressed mouse button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82e0b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585b944dabd040cf9a1a01122e401325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes3DSubplot:xlabel='$x_n[0]$', ylabel='$x_n[1]$'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(X,Y):\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], Y, marker='o', alpha=.8, label=r'examples $(\\mathbf{x}_n, y_n)$')\n",
    "    \n",
    "    ax.set_xlabel(r'$x_n[0]$')\n",
    "    ax.set_ylabel(r'$x_n[1]$')\n",
    "    ax.set_zlabel(r'$y$')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.view_init(elev=25, azim=10)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3575c",
   "metadata": {},
   "source": [
    "### Estimate Parameters of Linear Regression\n",
    "\n",
    "The parameters of the linear model are estimated by performing a linear regression. First the data matrix is extended in order to incorporate the intercept in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d8b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = np.concatenate((np.ones((len(X),1)), X), axis=1)\n",
    "theta_hat = np.linalg.inv(Xt.T @ Xt) @ Xt.T @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a19fe",
   "metadata": {},
   "source": [
    "Comparison of the estimated and true parameters reveals that the regression was able to determine the parameters with good accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03866ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated/true intercept: 1.259 / 1.200\n",
      "Estimated/true slopes: \n",
      "\t first dimension 0.091 / 0.100 \n",
      "\t second dimension 0.239 / 0.250\n"
     ]
    }
   ],
   "source": [
    "print('Estimated/true intercept: {0:.3f} / {1:.3f}'.format(theta_hat[0], alpha))\n",
    "print('Estimated/true slopes: \\n\\t first dimension {0:.3f} / {1:.3f} \\n\\t second dimension {2:.3f} / {3:.3f}'.format(\n",
    "    theta_hat[1], theta[0], theta_hat[2], theta[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dd372",
   "metadata": {},
   "source": [
    "A common visualization of linear regression is to show the data points together with the estimated linear predictor illustrated by a (regression) plane. Note, points on the regression plane constitute the output $\\hat{y}$ of the linear predictor for a given set of features $\\mathbf{x}$. Again, interactive exploration will aid in inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db4f5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7477dbf023f04d8da9ad69c0a869a7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd571f2a490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot data\n",
    "ax = plot_data(X, Y)\n",
    "\n",
    "# plot regression plane\n",
    "xx, yy = np.meshgrid(range(-7, 12), range(-7, 12))\n",
    "z = theta_hat[0] + theta_hat[1] * xx + theta_hat[2] * yy\n",
    "plane = ax.plot_surface(xx, yy, z, alpha=0.5, color='C1',\n",
    "                        label=r'regression plane')\n",
    "# see https://stackoverflow.com/questions/55531760/is-there-a-way-to-label-multiple-3d-surfaces-in-matplotlib\n",
    "plane._facecolors2d = plane._facecolor3d\n",
    "plane._edgecolors2d = plane._edgecolor3d\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130601fd",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The statistical properties of the residual error $e = y_n - \\hat{y}_n$ provide insights into the validity of the linear model and if the underlying assumptions are met. The [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$ is a common measure for the goodness-of-fit of an estimated model. Is bases on the total sum-of-squares and the residual sum-of-squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4822387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of residual error: \t\t -0.00000\n",
      "Standard deviation of residual error: \t 0.52108\n",
      "Total sum-of-squares (TSS): \t\t 279.85580\n",
      "Residual sum-of-squares (RSS): \t\t 54.30470\n",
      "Coefficient of determination: \t\t 0.80595\n"
     ]
    }
   ],
   "source": [
    "Y_hat = np.dot(Xt, theta_hat)\n",
    "e = Y - Y_hat\n",
    "mu_e = np.mean(e)\n",
    "std_e = np.std(e) \n",
    "\n",
    "TSS = np.sum((Y - np.mean(Y))**2)\n",
    "RSS = np.sum((Y-Y_hat)**2)\n",
    "Rs = 1 - RSS/TSS\n",
    "\n",
    "print('Mean of residual error: \\t\\t {0:.5f}'.format(mu_e))\n",
    "print('Standard deviation of residual error: \\t {0:.5f}'.format(std_e))\n",
    "print('Total sum-of-squares (TSS): \\t\\t {0:.5f}'.format(TSS))\n",
    "print('Residual sum-of-squares (RSS): \\t\\t {0:.5f}'.format(RSS))\n",
    "print('Coefficient of determination: \\t\\t {0:.5f}'.format(Rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd9d1a",
   "metadata": {},
   "source": [
    "**Copyright**\n",
    "\n",
    "This notebook is provided as [Open Educational Resource](https://de.wikipedia.org/wiki/Open_Educational_Resources).\n",
    "The text is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    ", the code of the IPython examples under the [MIT license](https://opensource.org/licenses/MIT). Please attribute the work as follows: Sascha Spors, Data driven audio signal processing - Lecture supplementals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
